


<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xml:lang="" lang="" version="-//W3C//DTD XHTML 1.1//EN" xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

  
  <title>Entrenamiento &mdash; documentaci√≥n de Husky Detection - 1.0</title>
  

  

  

    <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'/>
    <link rel="stylesheet" href="../_static/css/pdj.css" type="text/css" />

  
    <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />
  

  

  
    <link rel="stylesheet" href="../_static/css/darker.css" type="text/css" />
  
        <link rel="index" title="√çndice"
              href="../genindex.html"/>
        <link rel="search" title="B√∫squeda" href="../search.html"/>
    <link rel="top" title="documentaci√≥n de Husky Detection - 1.0" href="../index.html"/>
        <link rel="prev" title="Introducci√≥n" href="intro.html"/>

    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <meta http-equiv="cache-control" content="public" />
    <meta name="robots" content="follow, all" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Add jQuery library -->
    <script type="text/javascript" src="http://code.jquery.com/jquery-latest.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/modernizr/2.6.2/modernizr.min.js"></script>

  </head>

  <body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-nav-search">
        <a href="../index.html" class="fa fa-home"> Husky Detection </a>
        <div role="search">
	  <form id ="rtd-search-form" class="wy-form"
		action="../search.html" method="get">
	    <input type="text" name="q" placeholder="Search docs" />
	    <input type="hidden" name="check_keywords" value="yes" />
	    <input type="hidden" name="area" value="default" />
	  </form>
	</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	
          
          
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="intro.html">Introducci√≥n</a><ul>
<li class="toctree-l2"><a class="reference internal" href="intro.html#get-started">Get Started</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Entrenamiento</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#contexto">Contexto</a></li>
<li class="toctree-l2"><a class="reference internal" href="#instalacion">Instalaci√≥n</a></li>
</ul>
</li>
</ul>

          
        

      </div>
      &nbsp;
    </nav>
    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      <nav class="wy-nav-top" id="barra-mobile" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="#">Por√£o do Juca</a>
      </nav>

      <div class="wy-nav-content">
	<div class="fundo-claro">
	</div>
	<div class="fundo-escuro">
	</div>

        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
	    
	    <!-- <ul class="wy-breadcrumbs"> -->
	    <!--   <li><a href="#">Docs</a> &raquo;</li> -->

	    <!--   <li>Features</li> -->
	    <!--   <li class="wy-breadcrumbs-aside"> -->

	    <!-- 	<a href="_sources/index.txt" rel="nofollow"> View page source</a> -->

	    <!--   </li> -->
	    <!-- </ul> -->
	    <!-- <hr/> -->
	  </div>

          <div role="main" class="">

	    <div id="content" class="hfeed entry-container hentry">
  <div class="section" id="entrenamiento">
<h1>Entrenamiento<a class="headerlink" href="#entrenamiento" title="Enlazar permanentemente con este t√≠tulo">¬∂</a></h1>
<p>Repositorio: <a class="reference external" href="https://github.com/Daegas/CustumObjectDetection">CustumObjectDetection</a></p>
<div class="section" id="contexto">
<h2>Contexto<a class="headerlink" href="#contexto" title="Enlazar permanentemente con este t√≠tulo">¬∂</a></h2>
<p>El fin de esta secci√≥n es compilar informaci√≥n b√°sica para entender el proceso
de entrenamiento del modelo.</p>
<div class="section" id="redes-neuronales-convolucionales-cnn">
<h3>Redes Neuronales Convolucionales (CNN)<a class="headerlink" href="#redes-neuronales-convolucionales-cnn" title="Enlazar permanentemente con este t√≠tulo">¬∂</a></h3>
<p>Las CNN fueron introducidas por primera vez por Yann LeCun 1998 en
<a class="reference external" href="https://pdfs.semanticscholar.org/62d7/9ced441a6c78dfd161fb472c5769791192f6.pdf">Gradient-Based Learning Applied to Document Recognition</a>
y resultaron ser muy efectivas en reconocimiento de im√°genes y clasificaci√≥n.</p>
<p>üëÅ <a class="reference external" href="https://www.youtube.com/watch?v=aircAruvnKk">¬øPero qu√© ¬´es¬ª una Red neuronal?</a></p>
<p>C√≥mo se ve en el video una red neuronal est√° compuesta por capas, cada capa contiene un vector de
caracter√≠sticas que conservan una relaci√≥n espacial entre pixeles.
Dada una imagen en forma de matriz:</p>
<ul class="simple">
<li><p>üëÅ <a class="reference external" href="https://setosa.io/ev/image-kernels/">Explicaci√≥n Visual Kernels</a></p></li>
</ul>
<p>Se aplican filtros para resaltar bordes, circunferencias, esquinas, etc. Esta t√©cnica para detectar las partes m√°s importantes de una imagen se llama ¬´convoluci√≥n¬ª</p>
<ul class="simple">
<li><p>üëÅ <a class="reference external" href="https://www.youtube.com/watch?v=IHZwWFHWa-w&amp;t=660s">Descenso de gradiente, es como las redes neuronales aprenden</a></p></li>
</ul>
<p>Con el descenso de gradiente se buscar calcular cuales valores de pesos y bias minimizan el costo, la distancia del resultado obtenido en la √∫ltima capa con el resultado esperado.</p>
<ul class="simple">
<li><p>üëÅ <a class="reference external" href="https://www.youtube.com/watch?v=Ilg3gGewQ5U">¬øQu√© es la retropropagaci√≥n y qu√© hace en realidad?</a></p></li>
</ul>
<p>Cada neurona en la pen√∫ltima capa necesita unos ajustes de par√°metros (bias, pesos) para que en la √∫ltima capa (salida) las neuronas m√°s activas correspondan a las que se esperan est√©n m√°s activas. Se agregan todos los ajustes de todas las neuronas de esa pen√∫ltima capa, luego se hace lo mismo con la antepen√∫ltima y as√≠ hasta llegar a la segunda capa (la primera es la entrada). A eso se le llama ¬´retropropagaci√≥n¬ª.</p>
<p><strong>Otros datos:</strong></p>
<ul class="simple">
<li><p>üëÅ <a class="reference external" href="https://www.i2tutorials.com/activation-functions-in-deep-learning/">Funciones de Activaci√≥n</a></p></li>
</ul>
<p>Cada neurona toma la una entrada, la multiplica por el peso, le agrega el bias y el resultado lo pasa por una funci√≥n de activaci√≥n, que son funciones que determinan si una neurona debe o no ser activada. Ayudan a normalizar en un rango [0 , 1] o [-1 , 1] Existen muchos tipos de funciones de activaci√≥n (Sigmoid, ReLU, TanH, LeakyReLU, Softmax, etc)</p>
<ul class="simple">
<li><p>Capas</p></li>
</ul>
<p>El prop√≥sito de cada capa varia, a veces pueder ser reducir dimensionalidad (pooling), detectar patrones (convoluci√≥n) entre otras funciones.</p>
<ul class="simple">
<li><p>üëÅ <a class="reference external" href="https://medium.com/analytics-vidhya/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5">Arquitecturas</a></p></li>
</ul>
<p>Existen muchas arquitecturas, es decir cuantas capas, de que tipo y en que acomodo se usan en una red. Hay arquitecturas de redes neuronales que no tienen convoluci√≥n, generalmente las usadas en reconocimiento y clasificaci√≥n de im√°genes si tienen. Algunas arquitecturas muy famosas de CNN especiales para detectar patrones visuales son VGG, Inception, RESNET.</p>
<ul class="simple">
<li><p>üëÅ <a class="reference external" href="https://www.cs.cmu.edu/~aharley/vis/conv/flat.html">Aqu√≠</a> puedes encontrar un ejemplo interactivo y visual de una CNN para  reconocimiento de d√≠gitos.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Nota</p>
<p>No te preocupes si no entiendes al 100% todos los links o su teminolog√≠a, pero para los videos se recomienda verlos un par de veces para comprender los conceptos, ya que es la base del aprendizaje.</p>
</div>
</div>
<div class="section" id="deteccion-de-objetos">
<h3>Detecci√≥n de Objetos<a class="headerlink" href="#deteccion-de-objetos" title="Enlazar permanentemente con este t√≠tulo">¬∂</a></h3>
<p>De acuero con <a class="reference external" href="√Årearelacionadaconlavisi√≥nartificialyelprocesamientodeimagenquetratadedetectarcasosdeobjetossem√°nticosdeunaciertaclase(comohumanos,edificios,ocoches)env√≠deoseim√°genesdigitales.‚Äã">wikypedia</a>
la detecci√≥n de objetos es: √Årea relacionada con la visi√≥n artificial y el procesamiento de imagen que trata de detectar casos de objetos sem√°nticos de una cierta clase (como humanos, edificios, o coches) en v√≠deos e im√°genes digitales.‚Äã</p>
<p>Es diferente un problema de clasificaci√≥n c√≥mo:</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/fiat.jpg"><img alt="../_images/fiat.jpg" src="../_images/fiat.jpg" style="width: 200px;" /></a>
</div>
<p><a class="footnote-reference brackets" href="#f1" id="id1">1</a></p>
<p>Donde es una sola clase en toda la imagen; a un problema de reconocimiento y clasificaci√≥n como:</p>
<div class="figure align-left">
<a class="reference internal image-reference" href="../_images/or.png"><img alt="../_images/or.png" src="../_images/or.png" style="width: 200px;" /></a>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/or1.png"><img alt="../_images/or1.png" src="../_images/or1.png" style="width: 220px;" /></a>
</div>
<p>Que son varias instancias de varias clases en una sola im√°gen. Como podemos ver, para
el segundo ejemplo es necesario definir qu√© objeto se encuentra y que regi√≥n ocupa
dentro de la imagen.
Algunas arquitecturas que se usan para problemas como el segundo ejemplo son:</p>
<ul class="simple">
<li><p>Region Based Object Detectors como R-CNN, Fast RCNN, Faster R-CNN, R-FCN, FPN You can learn a bit more üëÅ`here &lt;<a class="reference external" href="https://medium.com/&#64;jonathan_hui/what-do-we-learn-from-region-based-object-detectors-faster-r-cnn-r-fcn-fpn-7e354377a7c9">https://medium.com/&#64;jonathan_hui/what-do-we-learn-from-region-based-object-detectors-faster-r-cnn-r-fcn-fpn-7e354377a7c9</a>&gt;`_</p></li>
</ul>
<p>Estas usan una ventana que mueven por toda la imagen, esta imagen de la ventana se ajusta a un tama√±o fijo
y a cada una se le aplica un clasificador. Para reducir el n√∫mero de operaciones, se usan m√©todos
que proponen Regiones de Inter√©s o  <em>Regions of Interest</em> (ROIs) o una red completa como <em>Region Proposal Network</em>
o RPN, que reducen el n√∫mero de ventanas a analizar,
lo cu√°l se traduce en menor tiempo de entrenamiento. Pero para regi√≥n se hacen <em>k</em> propuestas de regi√≥n
por clase. El n√∫mero de operaciones que se hacen sigue siendo elevado.</p>
<ul class="simple">
<li><p>üëÅ <a class="reference external" href="https://medium.com/&#64;jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d">Single Shot Detectors como YOLO y SSD</a></p></li>
</ul>
<p>C√≥mo se explica en el link, estas arquitecturas calculan al mismo tiempo un <em>boundary box</em> y la clase. Estos resultan ser buenos en procesamiento en tiempo real, pero tienenalgunos problemas
para detectar objetos o muy cercanos o muy lejanos.</p>
<p>Uno de los grandes problemas en detecci√≥n de objetos y en general de Machine Learning,
es la selecci√≥n del modelo. Esto se resuleve muchas veces de manera emp√≠rica o por
prueba y error. Vamos a tratar de implementar YOLO y SSD, sin dejar de tomar como opciones
otras arquitecturas.</p>
<p>Documentaci√≥n YOLO y SSD:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://pjreddie.com/darknet/yolo/">P√°gina oficial YOLO</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1506.02640">Art√≠culo YOLO</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1512.02325v5">Art√≠culo SSD</a></p></li>
</ul>
</div>
<div class="section" id="como-se-implementan">
<h3>¬øC√≥mo se implementan?<a class="headerlink" href="#como-se-implementan" title="Enlazar permanentemente con este t√≠tulo">¬∂</a></h3>
<p>Primero debemos configurar nuestro ambiente de desarrollo. C√≥mo se explic√≥ en la Introducci√≥n este debe
ser implentado en Ubuntu Focal. Lo que ocupamos:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.anaconda.com/">Anaconda</a> : Aunque no es necesaria, es s√∫per √∫til para crear ambientes con diferentes especificaciones. Adem√°s tiene otras herremientas para trabajar. Se pudieran usar otras herramientas como pipenv.</p></li>
<li><p><a class="reference external" href="https://www.tensorflow.org/">Tensorflow</a> Es una plataforma que contiene herramientas, librer√≠as y recursos que permiten a los desarrolladores introducirse al estado del arte en Machine Learning.‚Äã La implementaci√≥n de arquitecturas de redes neuronales es relativamente f√°cil. ‚ÄãCon 3 l√≠neas de c√≥digo es posible agregar capas. Basta con cambiar las entradas (placeholders) para usar en otras aplicaciones.‚Äã</p></li>
<li><p><a class="reference external" href="https://keras.io/">Keras</a>  es un API de alto nivel escrita en python para redes neuronales, permite trabajar por encima de Tensorflow. Ideal para hacer prototipos f√°ciles y r√°pidos.‚Äã Usa el backend de tensorflow y tiene ya implementadas capas que son comunes en muchas arquitecturas‚Äã</p></li>
</ul>
<p><strong>Hardware</strong></p>
<p>Es recomendable tener una tarjeta gr√°fica con poder computacional &gt;=3.5 para el entrenamiento.
Aunque se puede usar solo CPU, los tiempos de entrenamiento aumentan muy considerablemente.
Si se cuenta con tarjeta gr√°fica, estos son los requerimientos de hardware de acuerdo a la <a class="reference external" href="https://www.tensorflow.org/install/gpu#hardware_requirements">p√°gina
oficial de Tensorflow</a></p>
<ul class="simple">
<li><p><a class="reference external" href="https://developer.nvidia.com/cuda-gpus">Compute Capability Nvidia GPUs</a></p></li>
</ul>
<p>En res√∫men Tensorflow tiene soporte para CPU y GPU, es mil vices m√°s recomendable GPU, para la instalaci√≥n
de cualquiera de los dos se puede hacer por comandos pip o usando una imagen de docker. Nuevamente lo ideal
y en teor√≠a m√°s sencillo es con una imagen de docker, pues solo se tienen que instalar los drivers manualmente,</p>
<p>‚Äã</p>
</div>
</div>
<div class="section" id="instalacion">
<h2>Instalaci√≥n<a class="headerlink" href="#instalacion" title="Enlazar permanentemente con este t√≠tulo">¬∂</a></h2>
<div class="section" id="brief-summary">
<h3>Brief Summary<a class="headerlink" href="#brief-summary" title="Enlazar permanentemente con este t√≠tulo">¬∂</a></h3>
<p>√öltima actualizaci√≥n: 6/22/2019 with TensorFlow v1.13.1.</p>
<p>Pripalmente est√° basado en
<a class="reference external" href="https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10">el repositorio de EdjeElectronics</a>
y
<a class="reference external" href="https://github.com/Khaivdo/How-to-train-an-Object-Detector-using-Tensorflow-API-on-Ubuntu-16.04-GPU">el de Khaivdo</a>‚Äôs
repositories.</p>
<div class="admonition note">
<p class="admonition-title">Nota</p>
<p>El desarrollo se hizo con CPU, pues se contaba con una GPU antigua con poca capacidad de computo. Se anunciar√° que comandos pueden cambiar para diferente versi√≥n de Ubuntu o Tensorflow adem√°s de algunos links  de ayuda para la instalaci√≥n y algunos posibles problemas que se prevean.Aqu√≠ se us√≥ Tensorflow CPU 1.7</p>
</div>
</div>
<div class="section" id="id2">
<h3>1. Anaconda<a class="headerlink" href="#id2" title="Enlazar permanentemente con este t√≠tulo">¬∂</a></h3>
<p>Instalar los requerimientos:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sudo</span> <span class="n">apt</span><span class="o">-</span><span class="n">get</span> <span class="n">install</span> <span class="n">libgl1</span><span class="o">-</span><span class="n">mesa</span><span class="o">-</span><span class="n">glx</span> <span class="n">libegl1</span><span class="o">-</span><span class="n">mesa</span> <span class="n">libxrandr2</span> <span class="n">libxrandr2</span> <span class="n">libxss1</span> <span class="n">libxcursor1</span> <span class="n">libxcomposite1</span> <span class="n">libasound2</span> <span class="n">libxi6</span> <span class="n">libxtst6</span> <span class="o">-</span><span class="n">y</span>
</pre></div>
</div>
<p>Descarga el archivo de instalaci√≥n con estos comandos:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cd</span>  <span class="o">~/</span><span class="n">Desktop</span>
<span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">repo</span><span class="o">.</span><span class="n">anaconda</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">archive</span><span class="o">/</span><span class="n">Anaconda3</span><span class="o">-</span><span class="mf">2020.02</span><span class="o">-</span><span class="n">Linux</span><span class="o">-</span><span class="n">x86_64</span><span class="o">.</span><span class="n">sh</span>
<span class="n">chmod</span> <span class="o">+</span><span class="n">x</span> <span class="n">Anaconda3</span><span class="o">-</span><span class="mf">2020.02</span><span class="o">-</span><span class="n">Linux</span><span class="o">-</span><span class="n">x86_64</span><span class="o">.</span><span class="n">sh</span>
</pre></div>
</div>
<p>Y ejecuta:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sh</span> <span class="n">Anaconda3</span><span class="o">-</span><span class="mf">2020.02</span><span class="o">-</span><span class="n">Linux</span><span class="o">-</span><span class="n">x86_64</span><span class="o">.</span><span class="n">sh</span> <span class="o">-</span><span class="n">y</span>
</pre></div>
</div>
<p>Ahora que est√° instalado, puedes borrar el archivo:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">rm</span> <span class="n">Anaconda3</span><span class="o">-</span><span class="mf">2020.02</span><span class="o">-</span><span class="n">Linux</span><span class="o">-</span><span class="n">x86_64</span><span class="o">.</span><span class="n">sh</span>
</pre></div>
</div>
</div>
<div class="section" id="create-and-set-your-environment">
<h3>2. Create and set your environment<a class="headerlink" href="#create-and-set-your-environment" title="Enlazar permanentemente con este t√≠tulo">¬∂</a></h3>
<p>Open a new terminal and it should appear <em>(base)</em> before your user name.</p>
<p><em>I case it doesn‚Äôt, run this commands:</em> <sub>~</sub> eval
¬´$(/home//anaconda3/bin/conda shell.bash hook) conda init <sub>~</sub> <em>In
case you use a different shell, replace shell.bash for shell.&lt;
YourShell&gt;</em></p>
<p>Download the spec-list_tf-cpu.txt file
<a class="reference external" href="https://ugtomx-my.sharepoint.com/:f:/g/personal/de_gamasandoval_ugto_mx/EpCz_C7gow5Ai7OjD9TBcoABi6aGlIjGSsUvc4n5Gj3mdA?e=VbZKWV">here</a>:
Now let‚Äôs create and activate our environment: <sub>~</sub> conda create
‚Äìname tf-cpu ‚Äìfile ~/Downloads/spec-list_tf-cpu.txt conda activate
tf-cpu <sub>~</sub> Install this dependecies: <sub>~</sub> pip install Cython
pip install contextlib2 pip install pillow pip install lxml pip install
jupyter pip install matplotlib pip install pandas pip install
opencv-python pip install
¬´git+https://github.com/philferriere/cocoapi.git#egg=pycocotools&amp;subdirectory=PythonAPI¬ª
<sub>~</sub> Install tensorflow version 1.7 <sub>~</sub> pip install
tensorflow==1.7 <sub>~</sub> <em>You can propably use another version but some
lines of code might change, and you‚Äôll need to find its corresponding
API version</em></p>
</div>
<div class="section" id="download-repositories">
<h3>3. Download repositories<a class="headerlink" href="#download-repositories" title="Enlazar permanentemente con este t√≠tulo">¬∂</a></h3>
<p>First create a directory on your Desktop <sub>~</sub> cd ~/Desktop mkdir
ObjectDetection <sub>~</sub> ### 3.3 <a class="reference external" href="https://github.com/Daegas/CustumObjectDetection">This
Repository</a></p>
<div class="section" id="tensorflow-object-detection-api">
<h4>3.2 <a class="reference external" href="https://github.com/tensorflow/models">Tensorflow Object Detection API</a><a class="headerlink" href="#tensorflow-object-detection-api" title="Enlazar permanentemente con este t√≠tulo">¬∂</a></h4>
<p>There are several branches of the API they are targeted to different
tensorflow versions, since we installed version 1.7,
<a class="reference external" href="https://github.com/tensorflow/models/tree/adfd5a3aca41638aa9fb297c5095f33d64446d8f">here</a>
is the corresponding branch. You have two option for download it: -
Directly from github (Click on Clone or Download button) and then
extract it on your ~/Desktop/ObjectDetection directory. - Or you can try
with these commands, which I saw works properly <sub>~</sub> cd
~/Desktop/ObjectDetection git clone <a class="reference external" href="https://github.com/tensorflow/models">https://github.com/tensorflow/models</a>
cd models git reset ‚Äìhard adfd5a3aca41638aa9fb297c5095f33d64446d8f
<sub>~</sub> <em>You basically download the current repository and then reset
to an old commit with its sha.</em></p>
</div>
<div class="section" id="model-zoo">
<h4>3.3 <a class="reference external" href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md">Model Zoo</a><a class="headerlink" href="#model-zoo" title="Enlazar permanentemente con este t√≠tulo">¬∂</a></h4>
<p>You can find a list of models, download and extract into
PretrainedModels. Here will use
<a class="reference external" href="http://download.tensorflow.org/models/object_detection/ssd_inception_v2_coco_2018_01_28.tar.gz">ssd_inception_v2_coco</a>
Finally this is how ObjectDetection should look</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ADD</span> <span class="n">IMAGE</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="compile-protobufs">
<h3>4. Compile Protobufs<a class="headerlink" href="#compile-protobufs" title="Enlazar permanentemente con este t√≠tulo">¬∂</a></h3>
<p>Protobuf is a way to share data among applications, a little bit like
what JSON does. Is used by tensorflow to configure models and training
parameters and is implemented for several languages. So we need to
compile it for python. <sub>~</sub> cd
~/Desktop/ObjectDetection/models/research protoc
object_detection/protos/*.proto ‚Äìpython_out=. <sub>~</sub> This creates
a name_pb2.py file from every name.proto file in the
/object_detection/protos folder.</p>
<p><strong>(Note: TensorFlow occassionally adds new .proto files to the folder.
If you get an error saying ImportError: cannot import name
‚Äúsomething_something_pb2‚Äù , you may need to update the protoc command
to include the new .proto files.)</strong></p>
</div>
<div class="section" id="pythonpath">
<h3>5. PYTHONPATH<a class="headerlink" href="#pythonpath" title="Enlazar permanentemente con este t√≠tulo">¬∂</a></h3>
<p>For running, you need to specify where it gathers the data. So add
models/research to your PYTHONPATH. You‚Äôll need eat for each new
terminal, you could add it to your .bashrc file which is in /home and
appear by pressing <code class="docutils literal notranslate"><span class="pre">Ctrl</span></code>+<code class="docutils literal notranslate"><span class="pre">h</span></code> but you‚Äôll need to replace the
<code class="docutils literal notranslate"><span class="pre">pwd</span></code> for the absolute path to models/research.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>cd ~/Desktop/ObjectDetection/models/research/
export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim
</pre></div>
</div>
</div>
<div class="section" id="test">
<h3>6. Test<a class="headerlink" href="#test" title="Enlazar permanentemente con este t√≠tulo">¬∂</a></h3>
<p>There are 2 ways to test your installation ### Easy Just run: <sub>~</sub>
cd ~/Desktop/ObjectDetection/models/research/ python
object_detection/builders/model_builder_tf1_test.py <sub>~</sub> Looks
like this: ADD IMAGE</p>
<div class="section" id="explained">
<h4>Explained<a class="headerlink" href="#explained" title="Enlazar permanentemente con este t√≠tulo">¬∂</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cd</span> <span class="o">~/</span><span class="n">Desktop</span><span class="o">/</span><span class="n">ObjectDetection</span><span class="o">/</span><span class="n">models</span><span class="o">/</span><span class="n">research</span><span class="o">/</span><span class="n">object_detection</span>
<span class="n">jupyter</span> <span class="n">notebook</span> <span class="n">object_detection_tutorial</span><span class="o">.</span><span class="n">ipynb</span>
</pre></div>
</div>
<p>If it doesn‚Äôt opens directly the notebook, click on the link that
appears on your terminal and open the notebook
object_detection_tutorial.ipynb listed.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ADD</span> <span class="n">IMAGE</span>
</pre></div>
</div>
<p><em>This section is from
`Edje &lt;https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10&gt;`__</em></p>
<p>This opens the script in your default web browser and allows you to step
through the code one section at a time. You can step through each
section by clicking the ‚ÄúRun‚Äù button in the upper toolbar. The section
is done running when the ‚ÄúIn [ * ]‚Äù text next to the section populates
with a number (e.g. ‚ÄúIn [1]‚Äù).</p>
<p>(Note: part of the script downloads the ssd_mobilenet_v1 model from
GitHub, which is about 74MB. This means it will take some time to
complete the section, so be patient.)</p>
<p>Once you have stepped all the way through the script, you should see two
labeled images at the bottom section the page. If you see this, then
everything is working properly! If not, the bottom section will report
any errors encountered. See the
<a class="reference external" href="https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10#appendix-common-errors">Appendix</a>
for a list of errors I encountered while setting this up.</p>
<p><strong>Note: If you run the full Jupyter Notebook without getting any errors,
but the labeled pictures still don‚Äôt appear, try this: go in to
object_detection/utils/visualization_utils.py and comment out the
import statements around lines 29 and 30 that include matplotlib. Then,
try re-running the Jupyter notebook.</strong></p>
<p class="rubric">Footnotes</p>
<dl class="footnote brackets">
<dt class="label" id="f1"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p><a class="reference external" href="https://thumbs.dreamstime.com/b/old-fiat-500-1-13471810.jpg">https://thumbs.dreamstime.com/b/old-fiat-500-1-13471810.jpg</a></p>
</dd>
</dl>
</div>
</div>
</div>
</div>


	    </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="intro.html" class="btn btn-neutral" title="Introducci√≥n"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Daegas.
    </p>
  </div>

  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/jucacrispim/sphinx_pdj_theme">theme</a> provided by <a href="http://poraodojuca.net">Por√£o do Juca</a>.

</footer>
	</div>
	</div>
	  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'1.0',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
    <script type="text/javascript" src="../_static/jquery.js">

    </script>
    <script type="text/javascript" src="../_static/underscore.js">

    </script>
    <script type="text/javascript" src="../_static/doctools.js">

    </script>
    <script type="text/javascript" src="../_static/language_data.js">

    </script>
    <script type="text/javascript" src="../_static/translations.js">

    </script>

  

   <script type="text/javascript"
           src="../_static/js/theme.js"></script>

   <script type="text/javascript"
           src="../_static/js/pdj.js"></script>

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

  </body>
</html>